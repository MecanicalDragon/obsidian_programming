**Kafka**, as a distributed system, runs in a cluster. Each node in the cluster is called a Kafka *broker*. Kafka topics are divided into *partitions*. Partitions allow to parallelize message processing in a topic by splitting it across multiple brokers. That way publishing\consuming to\from each partition can be completely parallel. When a new event is published to a topic, it is appended to one of the topic's partitions. Events with the same *event key* are written to the same partition; Kafka guarantees that any consumer of a given topic-partition will always read that partition's events in the same order as they were written. Each consumer has its own offset in topic-partition, that is stored on broker-side, so different consumers can read single topic-partition with their own preferences, and each message will be delivered to all consumers in the order it was written to Kafka. Furthermore, Kafka keeps messages for some time, so new consumers can read old messages if they need.

Let’s imagine a topic with four partitions P1–P4. Two different producers publish, independently from each other, new events on the topic. Events with the same key are written to the same partition. Note that both producers can write to the same partition if appropriate. Kafka guarantees events consuming order within a single partition, but not among them. Note that one partition can be consumed by only one consumer in each consumer group, so if in one consumer group, there are more consumers than partitions in the topic, excess consumers will stay idle.

![[kafka_cluster.png]]

Internally, the topic-partition pair is a couple of files on a disk. This file-based storage format is called "Log Segments" and stores the messages and the offset information for each partition. Each Log Segment is a file that contains a set of messages, along with an index that maps the message offsets to their physical positions within the file. The index is stored in another file with a ".index" extension. When a consumer reads messages from a Kafka topic, it specifies the starting offset for each partition it consumes from. Kafka uses the index to efficiently seek to the position of the first message with the specified offset within the Log Segment, and then sequentially reads the remaining messages in the segment. When a Log Segment becomes too large, Kafka closes it, opens a new one, and updates the index to point to the new segment.

Kafka stores offsets of a consumer group in a compacted topic `__consumer_offsets`. This topic is partitioned and replicated like any other Kafka topic, and each partition contains a set of key-value pairs that represent the offsets of a consumer group for a specific topic partition. The offset value itself is stored as a 64-bit integer, while the key is a composite key that includes the group ID, topic name, and partition ID.

## Exactly Once

An idempotent operation is one which can be performed many times without carrying different effects. Kafka supports idempotence with Idempotent Producer / Exactly Once semantics. This means it ensures that messages published on Kafka topics should not be duplicated from the Producer side. Each producer has a unique Producer Id (PID). Producer includes its PID in the headers of every message sent to a broker. Additionally, each message receives an atomically increasing sequence number. A separate sequence exists for each topic-partition pair that a producer sends messages to. The broker rejects a producer request if its sequence number is not exactly one greater than the last committed message from that PID/Topic Partition pair. This ensures that, even though a producer must retry requests upon failures, every message will be persisted in the log exactly once. To achieve exactly-once delivery, on the Consumer side we must either transactionally acknowledge messages after handling or track messages by preemptively assigned unique message ids, saving them into the database and checking for id existence during message consumption.

[Kafka partitions](https://medium.com/event-driven-utopia/understanding-kafka-topic-partitions-ae40f80552e8) and [Kafka idempotent producer and consumer](https://medium.com/@shesh.soft/kafka-idempotent-producer-and-consumer-25c52402ceb9)
[Incremental Fetch Session Cache](https://cwiki.apache.org/confluence/display/KAFKA/KIP-227%3A+Introduce+Incremental+FetchRequests+to+Increase+Partition+Scalability)

## How to select a partitions number

- First of all, the number of partitions should be not fewer than the expected number of consumer instances at the peak load moment, otherwise excess instances will stay idle.
- Additionally, think about data distribution among partitions. If a producer sends a lot of messages with a particular key to a single partition, processing of messages with distinct keys in this partition may starve. An increased number of partitions will fix it. Furthermore, it will provide more even distribution of messages among consumer instances (if one partition has much more messages than others).
- Normally, it’s ok just to double the number of brokers in a cluster for normal throughput. But if you have high throughput or expect it to increase in perspective, or extend a cluster, it’s better to do it 3 times the number of brokers so you can plan and adjust in the future. Kafka inner topic `__consumer_offsets` has 50 partitions. Anyway, if you need 20 consumers at peak time you definitely need at least 20 partitions in topic. But don’t create a topic with 100 partitions, unless you have a very good reason.
- Also, it is a bad idea to use prime numbers because prime numbers are very difficult to divide among different numbers of brokers and consumers. 12 is a good guideline for the number of partitions. For customers who process very little data with Kafka, even smaller numbers make sense (2,4,6)
- If the workload is able to hit the network throughput limits, a rough formula for picking the number of partitions based on  a throughput can be picked. First, you measure the throughput you can achieve on a single partition for production (call it p) and consumption (call it c). Let’s say your target throughput is t. Then you need to have at least `max(t/p, t/c)` partitions.
- As a rule of thumb, if you care about latency, it’s probably a good idea to limit the number of partitions per broker to `100*b*r`, where b is the number of brokers in a Kafka cluster and r is the replication factor.
- It’s pretty much accepted that a broker *should not hold more than 4000 partitions* across all topics of a single broker.
- Additionally, a Kafka cluster should have a *maximum of 200,000 partitions* across all brokers, because in this case if a broker goes down, Zookeeper has to perform *a lot of leader elections*. If you need more than 200,000 partitions in a cluster, it’s better to follow the *Netflix model* and create more Kafka clusters.

## Drawbacks of large number of partitions

- Kafka opens two files for each partition: the log and the index. This has little impact on performance, but we should care about the number of open files on the operating system side. According to [this](https://github.com/apache/kafka/blob/trunk/docs/ops.html) and [this](http://kernel.org/doc/Documentation/sysctl/vm.txt) article Kafka uses a separate memory area for each of these files, and the maximum number of memory map areas a process may have in Linux (`vm.max_map_count`) is 65535. Kafka quote: *each partition requires minimum 2 map areas, as long as it hosts a single log segment. That is to say, creating 50000 partitions on a broker will result allocation of 100000 map areas and likely cause broker crash with OutOfMemoryError (Map failed) on a system with default vm.max_map_count. Keep in mind that the number of log segments per partition varies depending on the segment size, load intensity, retention policy and, generally, tends to be more than one*. In other words, with the default `vm.max_map_count` we can only have 32 topics with 1000 partitions, or 160 topics with 200 partitions, or 640 topics with 50 partitions. But in Amazon AWS Kafka has the max file limit per Linux process of 262,144 on all kinds of MSK clusters.
- If Kafka broker shuts down cleanly, it notifies the controller, and the controller can move the partition leaders to the other brokers without downtime. However, if a broker fails, there may be a long downtime because there is no leader for many partitions. Due to limitations in Zookeeper, the consumer can only move one leader at a time. With thousands of leaders to elect, this can take minutes + time to detect a failure. If one is unlucky, the failed broker may be the controller. In this case, the process of electing new leaders won’t start until the controller fails over to a new broker. The controller failover happens automatically but requires the new controller to read some metadata for every partition from Zookeeper during initialization. So, if the partition number is high, it can take additional significant time.
- Kafka clients create buffers per partition: clients accumulate messages for\from partition and send\fetch them in a batch. If a client interacts with a lot of partitions (especially if it’s a producer), then the RAM consumption increases a lot.
- More partitions may increase end-to-end latency. Kafka only exposes a message to a consumer after it has been committed, i.e., when the message is replicated to all the in-sync replicas. So, the time to commit a message can be a significant portion of the end-to-end latency. By default, a Kafka broker only uses a single thread to replicate data from another broker. A large number of partitions may affect this time.
- A large number of partitions generates more requests inside the Kafka cluster, increases traffic and consumes more CPU (1.5 times requests increase, and 4 times CPU usage increase). Furthermore, this affects the duration of rebalancing the cluster.

## How to select a replication factor

- Should be at least two, three is recommended, maximum four. The higher the replication factor, the better system resilience, but also the higher overall latency, because Kafka allows consumers to consume a message only when it was committed, i.e., replicated properly. Never set it to 1 on prod, because if the broker goes down, partition becomes offline.
- The number of required acks for successful writing is configurable. But when `acks=all` with a r`eplication.factor=N` and `min.insync.replicas=M` (minimal number of alive replicas for the partition) we can tolerate `N-M` brokers going down for topic availability purposes. Otherwise, the cluster will stop accepting messages and start to throw *NotEnoughReplicasExceptions*.
- `acks=all` and `min.insync.replicas=2` is the most popular option for data durability and availability and allows you to withstand at most the loss of one Kafka broker.

[Source](https://www.xeotek.com/how-many-partitions-do-i-need-in-apache-kafka/#:~:text=maximum%20200%2C000%20partitions%20per%20Kafka,50%20brokers%20per%20Kafka%20cluster) and [source](https://www.confluent.io/blog/how-choose-number-topics-partitions-kafka-cluster/) and [source](https://www.conduktor.io/kafka/kafka-topics-choosing-the-replication-factor-and-partitions-count)
